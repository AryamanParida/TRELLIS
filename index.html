<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Structured 3D Latents for Scalable and Versatile 3D Generation</title>
        <link rel="icon" href="/favicon.png">
        <link rel="stylesheet" href="./fonts/avenir-next/stylesheet.css">
        <link rel="stylesheet" href="./icons/style.css">
        <link rel="stylesheet" href="./css/main.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    </head>
    <body>
        <div id="main">
            <div id="teaser">
                <video autoplay loop muted src="assets/logo.mp4"></video>*
            </div>
            <div id="title">
                <span style="font-size: 4rem;">Structured 3D Latents</span><br>
                <span style="font-size: 2.2rem;">for Scalable and Versatile 3D Generation</span>
            </div>
            <div id="authors">
                <div><a class="link" href="https://jeffreyxiang.github.io">Jianfeng Xiang</a><sup>1,2</sup></div>
                <div><a class="link" href="https://github.com/MaxtirError">Zelong Lv</a><sup>3,2</sup></div>
                <div><a class="link" href="https://github.com/sicxu">Sicheng Xu</a><sup>2</sup></div>
                <div><a class="link" href="https://yudeng.github.io">Yu Deng</a><sup>2</sup></div>
                <div><a class="link" href="https://wangrc.site">Ruicheng Wang</a><sup>3,2</sup></div>
                <div><a class="link" href="http://home.ustc.edu.cn/~zhangbowen">Bowen Zhang</a><sup>3,2</sup></div>
                <div><a class="link" href="http://www.dongchen.pro/">Dong Chen</a><sup>2</sup></div>
                <div><a class="link" href="https://scholar.google.com/citations?user=P91a-UQAAAAJ&hl=en">Xin Tong</a><sup>2</sup></div>
                <div><a class="link" href="https://jlyang.org">Jiaolong Yang</a><sup>2</sup></div>
            </div>
            <div id="institution">
                <div><sup>1</sup><a class="link" href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a></div>
                <div><sup>2</sup><a class="link" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a></div>
                <div><sup>3</sup><a class="link" href="https://en.ustc.edu.cn/">USTC</a></div>
            </div>
            <div class="note">
                <i>* Generated by</i> <span style="font-size: 16px; font-weight: 600;">T</span><span style="font-size: 12px; font-weight: 700;">RELLIS</span>, <i>using functions</i> <span style="font-size: 16px; font-weight: 600;">image to 3D assets</span> <i>and</i> <span style="font-size: 16px; font-weight: 600;">asset variations</span>.
            </div>
            <div id="abstract">
                We introduce a novel 3D generation method for versatile and high-quality 3D asset creation.
                The cornerstone is a unified Structured LATent (<span style="font-size: 16px; font-weight: 600;">SL</span><span style="font-size: 12px; font-weight: 700;">AT</span>) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes.
                This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model,
                comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding.<br>
                We employ rectified flow transformers tailored for <span style="font-size: 16px; font-weight: 600;">SL</span><span style="font-size: 12px; font-weight: 700;">AT</span> as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects.
                Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales
                 We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models.
                 Code, model, and data will be released.
            </div>
            <div id="links">
                <div><a id="paper" href="https://arxiv.org/pdf/2303.17905.pdf">Paper</a></div>
                <div><a id="arxiv" href="https://arxiv.org/abs/2303.17905">Arxiv</a></div>
                <div><a id="code" href="https://github.com/JeffreyXiang/ivid">Code</a></div>
                <div><a id="poster" href="./ivid_poster_compressed.pdf">Poster</a></div>
            </div>            
            <div class="section">Citation</div>
            <p class="bibtex">
@inproceedings{xiang2023ivid,
    title     = {3D-aware Image Generation using 2D Diffusion Models},
    author    = {Xiang, Jianfeng and Yang, Jiaolong and Huang, Binbin and Tong, Xin},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {2383-2393}
}
            </p>
        </div>
        <div id="bottombar">
            <div>Structured 3D Latents for Scalable and Versatile 3D Generation</div>
            <div>Template designed by <span>Jianfeng Xiang</span>.</div>
        </div>
    </body>
</html>